{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e6c7cf-7b5b-4e76-825a-c3248885fb64",
   "metadata": {},
   "source": [
    "Multiple Linear regression & CART implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a3fd2-9e92-4dee-9701-ca0a297d7fe9",
   "metadata": {},
   "source": [
    "Packages needed for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca85d122-41eb-4491-879b-1d389c053359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f08bda-94c3-42f6-8680-cb46b495aa4e",
   "metadata": {},
   "source": [
    "PREPROCESSING DATA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e896fefe-c643-4307-af28-4f37100aba27",
   "metadata": {},
   "source": [
    "1. Firstly we load the data into Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "033b5f2b-1d3f-453a-9b95-07830847038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "redwine = pd.read_csv(r'C:\\Users\\JoAnN\\OneDrive\\Skrivbord\\MASTER\\Artificial intelligence for data science\\Python\\winequality-red.csv', sep=';')\n",
    "whitewine = pd.read_csv(R'C:\\Users\\JoAnN\\OneDrive\\Skrivbord\\MASTER\\Artificial intelligence for data science\\Python\\winequality-white.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59819b2-bcc1-4f86-a6ca-5973f89fcff3",
   "metadata": {},
   "source": [
    "2. Then we add a new feature, type, and concatenate the two files. We add -1 for red wine and 1 for white."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d6224d-ee1b-4585-9bc9-c05691422dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "redwine['type'] = 'red'\n",
    "whitewine['type'] = 'white'\n",
    "\n",
    "wine = pd.concat([redwine, whitewine], ignore_index=True)\n",
    "wine['type'] = wine['type'].map({'red': -1, 'white': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f02a835-480e-4923-8470-3a90a96ec08c",
   "metadata": {},
   "source": [
    "3. Then we wanna divide the data into target variable (y = quality) and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e044fa5-88b9-49a4-992a-efc433653bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wine.drop('quality', axis=1).values\n",
    "y = wine['quality'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed24da7-d997-4ef3-8410-0d7cc4b78643",
   "metadata": {},
   "source": [
    "4. Divide the data into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "386e3c56-63f3-4259-a7cc-029784a40ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f8e07b-2a8f-4620-9627-048f73c2a9bc",
   "metadata": {},
   "source": [
    "5. The data is scaled (standardized), meaning it will have a mean of 0 and standard deviation of 1. $(X- \\mu)/\\sigma$, where mu and sigma is for each column of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2931e1e-f655-4031-ad85-ef9f31c2feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1014792-778f-4d4f-bd66-27cd9a4007af",
   "metadata": {},
   "source": [
    "6. Add the \"bias\" or $\\theta_0$ to X, it will be a column of ones. OBS just run ones otherwise it gets incremented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fceb7d0-9e42-4e50-bce6-1b6f23718ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = np.c_[np.ones(X_train_scaled.shape[0]), X_train_scaled]\n",
    "X_test_scaled = np.c_[np.ones(X_test_scaled.shape[0]), X_test_scaled]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c3929d-cb7e-41b0-8e37-8a7871137398",
   "metadata": {},
   "source": [
    "\n",
    "MULTIPLE REGRESSION IMPLEMENTATION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29af763-5071-4e8a-a3e6-726588687b45",
   "metadata": {},
   "source": [
    " Gradient descent method\n",
    "\n",
    "Firstly we initialize the weights to zero, \"theta = np.zeros(n) \"\n",
    "\n",
    "\n",
    " Then we calculate the gradient vector of the costfunction\n",
    "\n",
    "$$ \\nabla_{\\theta} \\text{MSE}(\\boldsymbol{\\theta}) = \\frac{2}{m} \\boldsymbol{X}^T(\\boldsymbol{X} \\boldsymbol{\\theta} - \\boldsymbol{y}) $$\n",
    "\n",
    "Which corresponds to gradient = (2/m) * (X.T @ ( (X @ theta) - y ) )  in the code\n",
    "\n",
    "\n",
    "Update the weigthts \n",
    "\n",
    "$$ \\boldsymbol{\\theta}^{\\text{next step}} = \\boldsymbol{\\theta} - \\eta \\nabla_{\\boldsymbol{\\theta}} \\text{MSE}(\\boldsymbol{\\theta}) $$\n",
    "\n",
    "where $\\eta$ is the learning rate. This corresponds to theta = theta - eta * gradient in the code\n",
    "\n",
    "If the norm of the gradient is less than some epsilon (tolerance) we break the algorithm\n",
    "\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e94369be-7818-4b0a-9765-210dd1c23542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, eta, iterations, tol):\n",
    "    m, n = X.shape  # Number of observations and features\n",
    "    theta = np.zeros(n)  # Initialize weights\n",
    "    for i in range(iterations):\n",
    "        gradient = (2/m) * (X.T @ ( (X @ theta) - y ) )  #  gradient\n",
    "        theta = theta - eta * gradient  # Uppdate weights\n",
    "        \n",
    "        if np.linalg.norm(gradient) < tol:\n",
    "            print(f\"Converged at iteration {i}\")\n",
    "            break\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20641b0-c0ef-49e9-b682-dd49ae1c38aa",
   "metadata": {},
   "source": [
    "FIND THE WEIGHTS \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5f727-669e-4c58-a881-aa5f6e1ebd5c",
   "metadata": {},
   "source": [
    "2. Thereafter we set the learning rate, iteration and tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1df18fe6-9e6b-449c-b128-b795987031c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: [ 5.82547623  0.04124991 -0.25129553 -0.01200046  0.20264535 -0.02590926\n",
      "  0.09610745 -0.09064641 -0.15089026  0.0340718   0.10019783  0.34352783\n",
      " -0.10776505]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "iterations = 1000\n",
    "tol = 0.0001\n",
    "theta_hat = gradient_descent(X_train_scaled, y_train, learning_rate, iterations, tol)\n",
    "\n",
    "print(\"Theta:\", theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e8dba1-593b-4acc-9a45-af72a4e80611",
   "metadata": {},
   "source": [
    "ALMOST the same results the standard formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d88c3c7b-d763-407f-bf99-4ffc76c67203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.82547624  0.09245014 -0.24936989 -0.01025915  0.29070699 -0.02290224\n",
      "  0.08747713 -0.07533195 -0.29238194  0.06413342  0.10800171  0.28318006\n",
      " -0.15992148]\n"
     ]
    }
   ],
   "source": [
    "A = np.linalg.inv(np.dot(np.transpose(X_train_scaled),X_train_scaled)) \n",
    "B = np.dot(np.transpose(X_train_scaled),y_train)\n",
    "\n",
    "beta = np.dot(A,B)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e452575-d646-43c2-b1c2-ed8e4c4c2252",
   "metadata": {},
   "source": [
    "3. Make predictions $ \\hat{y} = \\mathbf{X}_{\\text{test}} \\cdot \\boldsymbol{\\theta} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87057d9d-7f82-42f4-a7c4-01dd1bc0b819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.62077808 5.54972371 5.42764299 ... 5.26335164 5.37407471 6.10852965]\n"
     ]
    }
   ],
   "source": [
    "yhat = X_test_scaled @ theta_hat\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec469c6-4d42-4423-8e86-2462416c0b25",
   "metadata": {},
   "source": [
    "Evaluation metrics, MAE, Accuracy, MEAM, CEMord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "561c2d5f-af16-44a1-856e-eafd82966af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mae(y_test, yhat):\n",
    "    # Make sure lengths match\n",
    "    if len(y_test) != len(yhat):\n",
    "        raise ValueError(\"Length of y_test and yhat must be the same\")\n",
    "    \n",
    "    # Calculate absolute differences\n",
    "    abs_diff = [abs(y - yhat_i) for y, yhat_i in zip(y_test, yhat)]\n",
    "    \n",
    "    # Calculate mean\n",
    "    mae = sum(abs_diff) / len(y_test)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "def calculate_accuracy(y_test, yhat):\n",
    "    \"\"\"\n",
    "    Accuracy as defined in paper: (1/n)∑(1 if y_i = ŷ_i, 0 otherwise)\n",
    "    \"\"\"\n",
    "    # Round predictions to nearest integer since wine quality is discrete\n",
    "    yhat_rounded = np.round(yhat)\n",
    "    correct = sum(1 for y, y_pred in zip(y_test, yhat_rounded) if y == y_pred)\n",
    "    return correct / len(y_test)\n",
    "\n",
    "def calculate_mae_m(y_test, yhat):\n",
    "    \"\"\"\n",
    "    Macro Averaged MAE as defined in paper:\n",
    "    MAEM = (1/#K)∑(MAEμ for each class)\n",
    "    \"\"\"\n",
    "    # Get unique classes\n",
    "    classes = np.unique(y_test)\n",
    "    \n",
    "    # Calculate MAE for each class\n",
    "    class_maes = []\n",
    "    for k in classes:\n",
    "        # Get indices where true value is class k\n",
    "        class_indices = y_test == k\n",
    "        if sum(class_indices) > 0:  # Only if we have samples of this class\n",
    "            mae_k = calculate_mae(y_test[class_indices], yhat[class_indices])\n",
    "            class_maes.append(mae_k)\n",
    "    \n",
    "    # Return average of class MAEs\n",
    "    return np.mean(class_maes)\n",
    "\n",
    "def calculate_cem_ord(y_test, yhat):\n",
    "    \"\"\"\n",
    "    CEMORD (Closeness Evaluation Measure) as defined in paper\n",
    "    \"\"\"\n",
    "    # Round predictions since wine quality is discrete\n",
    "    yhat_rounded = np.round(yhat)\n",
    "    \n",
    "    # Get unique classes\n",
    "    classes = np.unique(y_test)\n",
    "    n_classes = len(classes)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = np.zeros((n_classes, n_classes))\n",
    "    for i, j in zip(y_test, yhat_rounded):\n",
    "        i_idx = np.where(classes == i)[0][0]\n",
    "        j_idx = np.where(classes == j)[0][0]\n",
    "        cm[i_idx, j_idx] += 1\n",
    "    \n",
    "    # Calculate proximity for each pair of classes\n",
    "    def prox(k1, k2):\n",
    "        k1_count = sum(y_test == k1)\n",
    "        between_count = sum((y_test >= min(k1, k2)) & (y_test <= max(k1, k2)))\n",
    "        total = len(y_test)\n",
    "        return -np.log2((k1_count/2 + between_count) / total)\n",
    "    \n",
    "    # Calculate numerator and denominator\n",
    "    num = 0\n",
    "    den = 0\n",
    "    for i in classes:\n",
    "        for j in classes:\n",
    "            num += prox(i, j) * cm[np.where(classes == i)[0][0], np.where(classes == j)[0][0]]\n",
    "            if i == j:\n",
    "                den += prox(i, i) * sum(y_test == i)\n",
    "    \n",
    "    return num/den if den != 0 else 0\n",
    "\n",
    "# Update your cross validation function to include all metrics\n",
    "def evaluate_all_metrics(y_test, yhat):\n",
    "    \"\"\"\n",
    "    Calculate all performance metrics\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'mae': calculate_mae(y_test, yhat),\n",
    "        'accuracy': calculate_accuracy(y_test, yhat),\n",
    "        'mae_m': calculate_mae_m(y_test, yhat),\n",
    "        'cem_ord': calculate_cem_ord(y_test, yhat)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2937a3e-e23f-4723-9541-1e36975d90d8",
   "metadata": {},
   "source": [
    "5 fold cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "108b15c6-b172-4824-ab56-59afc6964cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, y,func_in, model_params={}, k_folds=5,random_state = 42, learning_rate=0.01, iterations=1000, tol=0.0001):\n",
    "    # Initialize metrics lists\n",
    "    metrics_lists = {\n",
    "        'mae': [], 'accuracy': [], 'mae_m': [], 'cem_ord': []\n",
    "    }\n",
    "    \n",
    "    # Get total number of samples\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    # Create random permutation of indices\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    \n",
    "    # Calculate fold size\n",
    "    fold_size = n_samples // k_folds\n",
    "    \n",
    "    for fold in range(k_folds):\n",
    "        # Calculate start and end indices for validation fold\n",
    "        val_start = fold * fold_size\n",
    "        val_end = (fold + 1) * fold_size if fold < k_folds - 1 else n_samples\n",
    "        \n",
    "        # Get validation indices for this fold\n",
    "        val_indices = indices[val_start:val_end]\n",
    "        \n",
    "        # Get training indices (all indices except validation)\n",
    "        train_indices = np.concatenate([indices[:val_start], indices[val_end:]])\n",
    "        \n",
    "        # Split data into training and validation using indices\n",
    "        X_train, X_val = X[train_indices], X[val_indices]\n",
    "        y_train, y_val = y[train_indices], y[val_indices]\n",
    "        \n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        \n",
    "        y_pred= func_in(X_train_scaled,X_val_scaled, y_train,**model_params)\n",
    "                \n",
    "        # Calculate all metrics\n",
    "        fold_metrics = evaluate_all_metrics(y_val, y_pred)\n",
    "        \n",
    "        # Store metrics\n",
    "        for metric_name, value in fold_metrics.items():\n",
    "            metrics_lists[metric_name].append(value)\n",
    "        \n",
    "        # Print fold results\n",
    "        print(f\"\\nFold {fold + 1} results:\")\n",
    "        for metric_name, value in fold_metrics.items():\n",
    "            print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Calculate means and standard deviations\n",
    "    results = {}\n",
    "    print(\"\\nOverall results:\")\n",
    "    for metric_name, values in metrics_lists.items():\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        results[f'{metric_name}_mean'] = mean_val\n",
    "        results[f'{metric_name}_std'] = std_val\n",
    "        print(f\"{metric_name}: {mean_val:.4f} (±{std_val:.4f})\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dc65ee-0cd5-4818-baae-b0a60a219f66",
   "metadata": {},
   "source": [
    "CART REGRESSION TREE IMPLEMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dc91205b-fa79-44e5-9d63-631ba54bcc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CART_reg(X_train_scaled, X_val_scaled, y_train, max_depth=10):\n",
    "    \"\"\"CART model function with standardized interface for cross validation\"\"\"\n",
    "    tree = RegressionCART(max_depth=max_depth)\n",
    "    tree.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = tree.predict(X_val_scaled)\n",
    "    return y_pred\n",
    "\n",
    "class Node:\n",
    "    \"\"\"A Node with a constructor\"\"\"\n",
    "    def __init__(self, feature_index: Optional[int] = None, threshold: Optional[float] = None,\n",
    "                 left: Optional['Node'] = None, right: Optional['Node'] = None, value: Optional[float] = None):\n",
    "        self.feature_index = feature_index  # Which feature index is used to split data\n",
    "        self.threshold = threshold          # Threshold value for the split\n",
    "        self.left = left                    # Left child node\n",
    "        self.right = right                  # Right child node\n",
    "        self.value = value                  # If node is leaf, store the predicted value(mean) in value\n",
    "\n",
    "class RegressionCART:\n",
    "    \"\"\" CART class\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth: int = 10, min_samples_split: int = 2, min_samples_leaf: int = 1):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.max_depth = max_depth  # Maximum depth of tree, prevents overfitting\n",
    "        self.min_samples_split = min_samples_split # Minimum number of samples to split a node\n",
    "        self.min_samples_leaf = min_samples_leaf # Minimum number of samples required in a leaf\n",
    "        self.root = None # Root of tree\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\" Help function, calls grow_tree on the whole dataset and stores it in root.\"\"\"\n",
    "        self.n_features = X.shape[1]  # Store number of features\n",
    "        self.root = self._grow_tree(X, y)\n",
    "\n",
    "    def _grow_tree(self, X: np.ndarray, y: np.ndarray, depth: int = 0) -> Node:\n",
    "        \"\"\" Recursively grow the tree by splitting the data.\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Check stopping criteria\n",
    "        if (depth >= self.max_depth or \n",
    "            n_samples < self.min_samples_split or \n",
    "            len(np.unique(y)) == 1):\n",
    "            return Node(value=np.mean(y))\n",
    "        \n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "        \n",
    "        # If no valid split is found, create a leaf node\n",
    "        if best_feature is None:\n",
    "            return Node(value=np.mean(y))\n",
    "        \n",
    "        # Create the split masks\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        \n",
    "        # Split the data\n",
    "        X_left, y_left = X[left_mask], y[left_mask]\n",
    "        X_right, y_right = X[~left_mask], y[~left_mask]\n",
    "        \n",
    "        # Check minimum samples in leaves\n",
    "        if len(y_left) < self.min_samples_leaf or len(y_right) < self.min_samples_leaf:\n",
    "            return Node(value=np.mean(y))\n",
    "        \n",
    "        # Create node and recursively grow children\n",
    "        left_child = self._grow_tree(X_left, y_left, depth + 1)\n",
    "        right_child = self._grow_tree(X_right, y_right, depth + 1)\n",
    "        \n",
    "        return Node(\n",
    "            feature_index=best_feature,\n",
    "            threshold=best_threshold,\n",
    "            left=left_child,\n",
    "            right=right_child\n",
    "        )\n",
    "        \n",
    "    def _calculate_mse(self, y: np.ndarray) -> float:\n",
    "        \"\"\" Calculate the Mean Squared Error (MSE) for a node.\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0.0\n",
    "        return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "    def _find_best_split(self, X: np.ndarray, y: np.ndarray) -> Tuple[Optional[int], Optional[float]]:\n",
    "        \"\"\" Find the best split that minimizes the weighted MSE.\"\"\"\n",
    "        best_mse = float('inf')\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        n_samples = len(y)\n",
    "        \n",
    "        # Iterate through all features\n",
    "        for feature_idx in range(X.shape[1]):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            thresholds = np.unique(feature_values)\n",
    "            \n",
    "            # Try all possible thresholds\n",
    "            for threshold in thresholds:\n",
    "                # Create masks for the split\n",
    "                left_mask = feature_values <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                # Skip if split creates empty nodes\n",
    "                if not left_mask.any() or not right_mask.any():\n",
    "                    continue\n",
    "                \n",
    "                # Get the split data\n",
    "                y_left = y[left_mask]\n",
    "                y_right = y[right_mask]\n",
    "                \n",
    "                # Calculate the weighted MSE\n",
    "                n_left, n_right = len(y_left), len(y_right)\n",
    "                mse = (n_left * self._calculate_mse(y_left) + \n",
    "                      n_right * self._calculate_mse(y_right)) / n_samples\n",
    "                \n",
    "                # Update best split if this is better\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_feature = feature_idx\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict target values for samples in X.\"\"\"\n",
    "        return np.array([self._traverse_tree(x, self.root) for x in X])\n",
    "\n",
    "    def _traverse_tree(self, x: np.ndarray, node: Node) -> float:\n",
    "        \"\"\"Traverse the tree to make a prediction for a single sample.\"\"\"\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        \n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f642ab3-4428-41b8-9fcb-8c33d6d5cd2e",
   "metadata": {},
   "source": [
    "Multiple Linear regression function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d61b24-5c33-4cea-9b25-ff4b9b8c0662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_reg(X_train_scaled,X_val_scaled, y_train, learning_rate, iterations, tol):\n",
    "            # Add bias term\n",
    "        X_train_scaled = np.c_[np.ones(X_train_scaled.shape[0]), X_train_scaled]\n",
    "        X_val_scaled = np.c_[np.ones(X_val_scaled.shape[0]), X_val_scaled]\n",
    "        \n",
    "        # Train model\n",
    "        theta = gradient_descent(X_train_scaled, y_train, learning_rate, iterations, tol)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = X_val_scaled @ theta\n",
    "        return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96897d4f-c7bb-4857-b062-e13e58509d33",
   "metadata": {},
   "source": [
    "RUN MULTIPLE LIENAR REGRSSION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99b597-29b7-4b79-b881-6b27442d95f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "redwine = pd.read_csv(r'C:\\Users\\JoAnN\\OneDrive\\Skrivbord\\MASTER\\Artificial intelligence for data science\\Python\\winequality-red.csv', sep=';')\n",
    "whitewine = pd.read_csv(R'C:\\Users\\JoAnN\\OneDrive\\Skrivbord\\MASTER\\Artificial intelligence for data science\\Python\\winequality-white.csv', sep=';')\n",
    "\n",
    "redwine['type'] = 'red'\n",
    "whitewine['type'] = 'white'\n",
    "\n",
    "wine = pd.concat([redwine, whitewine], ignore_index=True)\n",
    "wine['type'] = wine['type'].map({'red': -1, 'white': 1})\n",
    "\n",
    "X = wine.drop('quality', axis=1).values\n",
    "y = wine['quality'].values\n",
    "mlr_params = {\n",
    "    'learning_rate': 0.01,\n",
    "    'iterations': 1000,\n",
    "    'tol': 0.0001\n",
    "}\n",
    "results = cross_validation(X, y, mul_reg, model_params=mlr_params,k_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6bb001-cc03-4cf2-a83d-b399898e5f55",
   "metadata": {},
   "source": [
    "RUN CART MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5f267906-b36e-4e4b-a5fe-e1e854e063d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 results:\n",
      "mae: 0.5630\n",
      "accuracy: 0.5335\n",
      "mae_m: 1.2995\n",
      "cem_ord: 0.5474\n",
      "\n",
      "Fold 2 results:\n",
      "mae: 0.5581\n",
      "accuracy: 0.5597\n",
      "mae_m: 1.1217\n",
      "cem_ord: 0.5679\n",
      "\n",
      "Fold 3 results:\n",
      "mae: 0.5647\n",
      "accuracy: 0.5466\n",
      "mae_m: 1.3387\n",
      "cem_ord: 0.5714\n",
      "\n",
      "Fold 4 results:\n",
      "mae: 0.5537\n",
      "accuracy: 0.5735\n",
      "mae_m: 1.2508\n",
      "cem_ord: 0.5758\n",
      "\n",
      "Fold 5 results:\n",
      "mae: 0.5329\n",
      "accuracy: 0.5688\n",
      "mae_m: 0.9836\n",
      "cem_ord: 0.5869\n",
      "\n",
      "Overall results:\n",
      "mae: 0.5545 (±0.0115)\n",
      "accuracy: 0.5564 (±0.0147)\n",
      "mae_m: 1.1988 (±0.1301)\n",
      "cem_ord: 0.5699 (±0.0129)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cart_params = {\n",
    "    'max_depth': 10\n",
    "}\n",
    "results_cart = cross_validation(X, y, CART_reg, model_params=cart_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
